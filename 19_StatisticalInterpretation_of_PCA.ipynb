{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA의 통계적 해석\n",
    "\n",
    "## 1. 공분산 행렬 (Covariance Matrix)\n",
    "\n",
    "- PCA는 데이터의 분산과 상관 구조를 분석하기 위해 **공분산 행렬**을 계산하는 것으로 시작합니다.\n",
    "- 주어진 데이터 행렬 X가 n × p 차원(즉, n개의 관측치와 p개의 변수)이라고 가정합니다.\n",
    "\n",
    "- 데이터 행렬 X는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### 1)데이터 중심화 (Mean-Centering)\n",
    "\n",
    "- PCA를 적용하기 전에, 각 변수의 평균을 0으로 맞추기 위해 **데이터 중심화**를 수행합니다. 중심화된 데이터 행렬 X_centered는 다음과 같이 계산됩니다:\n",
    "\n",
    "$$\n",
    "X_{centered} = X - \\mu\n",
    "$$\n",
    "\n",
    "- μ는 각 변수의 평균을 나타내는 p × 1 벡터입니다.\n",
    "\n",
    "### 2)공분산 행렬 계산\n",
    "\n",
    "중심화된 데이터 행렬 X_centered를 사용하여, 공분산 행렬 Σ를 계산합니다:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X_{centered}^T X_{centered}\n",
    "$$\n",
    "\n",
    "이 공분산 행렬은 p × p 크기의 대칭 행렬로, 각 원소 σ_ij는 변수 i와 j 사이의 공분산을 나타냅니다. 대각 원소 σ_ii는 변수 i의 분산을 나타냅니다.\n",
    "\n",
    "## 2. 고유값과 고유벡터 (Eigenvalues and Eigenvectors)\n",
    "\n",
    "- PCA는 공분산 행렬 Σ를 **고유분해(Eigendecomposition)**하여 고유값과 고유벡터를 구하는 과정입니다.\n",
    "\n",
    "### 1)고유값과 고유벡터 공식\n",
    "\n",
    "- 고유값 λ와 고유벡터 v는 다음의 고유방정식을 만족합니다:\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "- **고유값 λ**: 주성분이 설명하는 분산의 크기를 나타냅니다. 고유값이 클수록, 그 주성분이 데이터의 변동성을 많이 설명합니다.\n",
    "\n",
    "- **고유벡터 v**: 주성분의 방향을 나타내며, 원래 변수들의 선형 결합으로 표현됩니다. 즉, 고유벡터는 새로운 축의 방향을 정의합니다.\n",
    "\n",
    "### 2)고유값의 성질\n",
    "\n",
    "- 고유값들은 보통 내림차순으로 정렬됩니다.\n",
    "- 고유값의 합은 전체 데이터의 분산과 같습니다:\n",
    "\n",
    "  $$\n",
    "  \\text{Total Variance} = \\sum_{i=1}^{p} \\lambda_i\n",
    "  $$\n",
    "\n",
    "- 각 고유값에 대응하는 고유벡터는 주성분 방향을 정의합니다.\n",
    "\n",
    "\n",
    "## 3. 주성분 (Principal Components)\n",
    "\n",
    "- 각 고유벡터는 데이터의 새로운 축을 정의하며, 이 축들을 따라 데이터를 다시 표현합니다. \n",
    "\n",
    "### 1)주성분 변환\n",
    "\n",
    "- 주어진 데이터 행렬 X를 주성분 공간으로 변환하기 위해 고유벡터 행렬 V를 사용합니다:\n",
    "\n",
    "$$\n",
    "Z = X_{centered} V\n",
    "$$\n",
    "\n",
    "- Z는 변환된 데이터 행렬이며, 각 행은 주성분 좌표 공간에서의 관측치를 나타냅니다. V는 고유벡터들로 이루어진 p × p 행렬입니다.\n",
    "\n",
    "### 2)주성분의 해석\n",
    "\n",
    "- **첫 번째 주성분 (PC1)**: 데이터의 최대 변동성을 설명하는 방향입니다. 이는 가장 큰 고유값에 대응하는 고유벡터입니다.\n",
    "- **두 번째 주성분 (PC2)**: 첫 번째 주성분과 직교하며, 두 번째로 큰 변동성을 설명합니다. 첫 번째 주성분이 설명하지 못한 추가적인 변동성을 설명합니다.\n",
    "\n",
    "- 각 주성분은 데이터의 분산을 최대한 설명하면서 서로 직교(독립)하는 특성을 가집니다.\n",
    "\n",
    "\n",
    "## 4. 설명된 분산 (Explained Variance)\n",
    "\n",
    "### 1)분산 비율\n",
    "\n",
    "- 각 주성분이 설명하는 분산의 비율은 고유값 λ_i의 크기로 나타납니다. 전체 데이터의 분산은 고유값의 합과 같으므로, 특정 주성분이 설명하는 분산의 비율은 다음과 같이 계산됩니다:\n",
    "\n",
    "$$\n",
    "\\text{Explained Variance Ratio for } PC_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}\n",
    "$$\n",
    "\n",
    "- 이 비율은 특정 주성분이 데이터 전체의 변동성을 얼마나 설명하는지를 나타내며, 이를 통해 몇 개의 주성분만으로도 데이터의 대부분의 정보를 유지할 수 있는지 알 수 있습니다.\n",
    "\n",
    "\n",
    "## 5. PCA의 통계적 가정과 해석\n",
    "\n",
    "### 1)선형성 (Linearity)\n",
    "\n",
    "- PCA는 원래 변수들 간의 선형 관계를 가정합니다. 따라서 변수들 사이에 비선형 관계가 있는 경우, PCA는 이러한 관계를 잘 포착하지 못할 수 있습니다.\n",
    "\n",
    "### 2)잡음 제거 (Noise Filtering)\n",
    "\n",
    "- PCA는 주성분을 통해 분산이 큰 방향을 강조하고, 분산이 작은 방향(즉, 잡음이 포함될 가능성이 높은)을 억제합니다. 이를 통해 데이터에서 중요한 패턴을 추출하고 잡음을 제거할 수 있습니다.\n",
    "\n",
    "### 3)다중공선성 (Multicollinearity) 처리\n",
    "\n",
    "- PCA는 다중공선성이 있는 데이터셋에 특히 유용합니다. 공선성이 높은 변수들이 주성분으로 결합되면서, 서로 상관된 변수들을 하나의 주성분으로 요약할 수 있습니다. 이는 다중공선성을 효과적으로 줄여줍니다.\n",
    "\n",
    "\n",
    "## 6. PCA의 추가 해석\n",
    "\n",
    "### 1)스코어 (Scores)와 로딩 (Loadings)\n",
    "\n",
    "- **스코어 (Scores)**: 각 관측치의 주성분 공간에서의 좌표를 나타냅니다. 이는 새로운 축(주성분) 상에서 데이터가 어떻게 위치하는지를 보여줍니다.\n",
    "- **로딩 (Loadings)**: 원래 변수들이 주성분을 형성하는 데 기여하는 정도를 나타내며, 고유벡터의 요소들로 표현됩니다. 로딩은 각 주성분이 원래 변수들에서 어떤 방향으로 형성되었는지를 나타냅니다.\n",
    "\n",
    "### 2)주성분 점수와 원래 변수 간의 관계\n",
    "\n",
    "PCA에서 각 주성분 점수는 원래 데이터의 선형 결합입니다. 주성분 점수와 원래 변수 간의 관계는 다음과 같이 표현됩니다:\n",
    "\n",
    "$$\n",
    "Z_i = X \\cdot v_i\n",
    "$$\n",
    "\n",
    "여기서 Z_i는 i번째 주성분 점수, v_i는 i번째 고유벡터입니다. 이는 주성분이 원래 변수의 선형 조합으로 나타낼 수 있음을 보여줍니다.\n",
    "\n",
    "PCA는 고차원 데이터를 분석하고 시각화할 수 있는 강력한 도구입니다. 통계적 해석을 통해 데이터를 축약하고 요약하는 데 유용하며, 이를 통해 데이터의 구조와 주요 패턴을 이해할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
